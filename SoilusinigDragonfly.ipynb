{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30120212-c53d-40f3-aa7d-77c5a43e7b37",
   "metadata": {},
   "source": [
    "# Dragonfly Algorithm for CNN Hyperparameter Tuning: Soil Image Classification\n",
    "\n",
    "## ğŸ§  Project Overview\n",
    "\n",
    "This project utilizes the **Dragonfly Algorithm (DA)** to optimize hyperparameters of a **Convolutional Neural Network (CNN)** for **soil image classification**. The Dragonfly Algorithm is inspired by the static and dynamic swarming behaviors of dragonflies in nature and has proven effective in solving complex optimization tasks.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ Objectives\n",
    "\n",
    "- Employ the Dragonfly Algorithm to fine-tune CNN hyperparameters.\n",
    "- Improve the CNN modelâ€™s accuracy in classifying soil image types.\n",
    "- Benchmark performance against other metaheuristics like Hybrid PSO, WOA, and ALO.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ› ï¸ Methodology\n",
    "\n",
    "### 1. Dataset\n",
    "- Dataset includes soil images categorized into multiple classes.\n",
    "- Preprocessing steps: image resizing, normalization, label encoding.\n",
    "- Split into **training** and **testing** sets.\n",
    "\n",
    "### 2. CNN Architecture\n",
    "- Two convolutional layers with ReLU activation and max-pooling.\n",
    "- Dense layer followed by dropout for regularization.\n",
    "- Output layer uses Softmax for multi-class prediction.\n",
    "\n",
    "### 3. Hyperparameters Optimized\n",
    "- **Learning Rate**\n",
    "- **Number of Filters**\n",
    "- **Dropout Rate**\n",
    "\n",
    "### 4. Dragonfly Algorithm (DA)\n",
    "- Mimics static and dynamic swarming behaviors.\n",
    "- Dragonflies search for optimal hyperparameters by updating positions influenced by:\n",
    "  - **Separation** (avoidance of crowding),\n",
    "  - **Alignment** (velocity matching with neighbors),\n",
    "  - **Cohesion** (attraction toward the center of the neighborhood),\n",
    "  - **Attraction to Food Source** (best solution),\n",
    "  - **Distraction from Enemy** (worst solution).\n",
    "\n",
    "- Fitness is calculated using CNN validation accuracy after limited training epochs.\n",
    "- The dragonfly with the highest fitness is selected to train the final model.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“ˆ Evaluation Metrics\n",
    "\n",
    "- **Test Accuracy**: Measures prediction success on unseen test images.\n",
    "- **Confusion Matrix**: Visualizes prediction accuracy across all classes.\n",
    "- **ROC Curve (Optional)**: Illustrates performance trade-offs, especially for binary classes.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“Š Final Results\n",
    "\n",
    "- **Best Learning Rate**: *e.g., 0.0010*\n",
    "- **Best Number of Filters**: *e.g., 32*\n",
    "- **Best Dropout Rate**: *e.g., 0.25*\n",
    "- **Test Accuracy**: *e.g., 83.7%*\n",
    "\n",
    "> The Dragonfly Algorithm effectively improved CNN accuracy by guiding the model toward more optimal hyperparameter combinations.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“Œ Conclusion\n",
    "\n",
    "- The Dragonfly Algorithm is a competitive method for CNN hyperparameter tuning.\n",
    "- It offers strong performance on soil classification tasks.\n",
    "- DA adapts well to complex landscapes and high-dimensional search spaces.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”­ Future Work\n",
    "\n",
    "- Expand tuning to more CNN parameters like number of layers and batch size.\n",
    "- Explore hybrid approaches combining DA with PSO or Genetic Algorithms.\n",
    "- Apply DA to other domains like medical imaging, satellite classification, etc.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“ Repository Structure\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b8705cd-7a6f-40f2-a251-3675bbf1e273",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d19c698-b633-49ba-962d-9026b4767430",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2240 images belonging to 5 classes.\n",
      "Found 480 images belonging to 5 classes.\n",
      "Found 480 images belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "# Set image size and batch size\n",
    "img_height, img_width = 256,256\n",
    "batch_size = 64\n",
    "\n",
    "# Paths to your data\n",
    "train_dir = 'ProcessedSpilt_dataset/train'\n",
    "val_dir = 'ProcessedSpilt_dataset/val'\n",
    "test_dir = 'ProcessedSpilt_dataset/test'\n",
    "\n",
    "# Initialize data generators\n",
    "datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = datagen.flow_from_directory(train_dir, target_size=(img_height, img_width), batch_size=batch_size, class_mode='categorical')\n",
    "val_generator = datagen.flow_from_directory(val_dir, target_size=(img_height, img_width), batch_size=batch_size, class_mode='categorical')\n",
    "test_generator = datagen.flow_from_directory(test_dir, target_size=(img_height, img_width), batch_size=batch_size, class_mode='categorical', shuffle=False)\n",
    "\n",
    "num_classes = len(train_generator.class_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a0bbc12-809e-4124-a26b-bc9eadd9dc2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Fitness Function ===\n",
    "def evaluate_model(params):\n",
    "    lr, f1, f2, dense = params\n",
    "    f1, f2, dense = int(f1), int(f2), int(dense)\n",
    "\n",
    "    model = models.Sequential([\n",
    "        layers.Input(shape=(img_height, img_width, 3)),\n",
    "        layers.Conv2D(f1, (3, 3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D(2, 2),\n",
    "\n",
    "        layers.Conv2D(f2, (3, 3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D(2, 2),\n",
    "\n",
    "        layers.Conv2D(f2*2, (3, 3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D(2, 2),\n",
    "\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(dense, activation='relu'),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer=optimizers.Adam(learning_rate=lr),\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, verbose=0)\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=4, restore_best_weights=True)\n",
    "\n",
    "    history = model.fit(train_generator, \n",
    "                        validation_data=val_generator, \n",
    "                        epochs=10, \n",
    "                        verbose=0, \n",
    "                        callbacks=[lr_scheduler, early_stop])\n",
    "    return history.history['val_accuracy'][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4219a225-8fa3-4c7c-af6a-0d974a0bb0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Dragonfly Optimization ===\n",
    "def dragonfly_optimize(bounds, dim=4, pop_size=6, iterations=5):\n",
    "    positions = np.random.uniform([b[0] for b in bounds], [b[1] for b in bounds], size=(pop_size, dim))\n",
    "    best_score = -np.inf\n",
    "    best_position = None\n",
    "    convergence = []\n",
    "\n",
    "    for iter in range(iterations):\n",
    "        scores = []\n",
    "        for i in range(pop_size):\n",
    "            fitness = evaluate_model(positions[i])\n",
    "            scores.append(fitness)\n",
    "            if fitness > best_score:\n",
    "                best_score = fitness\n",
    "                best_position = positions[i].copy()\n",
    "        convergence.append(best_score)\n",
    "        print(f\"Iteration {iter+1}, Best Accuracy: {best_score:.4f}\")\n",
    "        \n",
    "        for i in range(pop_size):\n",
    "            positions[i] += np.random.uniform(-0.05, 0.05, dim)\n",
    "            positions[i] = np.clip(positions[i], [b[0] for b in bounds], [b[1] for b in bounds])\n",
    "\n",
    "    return best_position, best_score, convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7db6e12-14a1-4ae1-95c2-4729cbe87b45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-24 18:10:52.626204: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M1 Max\n",
      "2025-05-24 18:10:52.626244: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 32.00 GB\n",
      "2025-05-24 18:10:52.626249: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 10.67 GB\n",
      "2025-05-24 18:10:52.626272: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2025-05-24 18:10:52.626290: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "/Users/simarkalsi/Cd Project Soil/soil/lib/python3.12/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n",
      "2025-05-24 18:10:53.987840: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, Best Accuracy: 0.4000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      1\u001b[39m bounds = [\n\u001b[32m      2\u001b[39m     [\u001b[32m0.00005\u001b[39m, \u001b[32m0.001\u001b[39m],   \u001b[38;5;66;03m# learning rate (slightly smaller upper bound)\u001b[39;00m\n\u001b[32m      3\u001b[39m     [\u001b[32m64\u001b[39m, \u001b[32m128\u001b[39m],          \u001b[38;5;66;03m# conv1 filters\u001b[39;00m\n\u001b[32m      4\u001b[39m     [\u001b[32m128\u001b[39m, \u001b[32m256\u001b[39m],         \u001b[38;5;66;03m# conv2 filters\u001b[39;00m\n\u001b[32m      5\u001b[39m     [\u001b[32m256\u001b[39m, \u001b[32m512\u001b[39m]          \u001b[38;5;66;03m# dense units\u001b[39;00m\n\u001b[32m      6\u001b[39m ]\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m best_params, best_score, convergence = \u001b[43mdragonfly_optimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbounds\u001b[49m\u001b[43m)\u001b[49m \n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 11\u001b[39m, in \u001b[36mdragonfly_optimize\u001b[39m\u001b[34m(bounds, dim, pop_size, iterations)\u001b[39m\n\u001b[32m      9\u001b[39m scores = []\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(pop_size):\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m     fitness = \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpositions\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m     scores.append(fitness)\n\u001b[32m     13\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m fitness > best_score:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 33\u001b[39m, in \u001b[36mevaluate_model\u001b[39m\u001b[34m(params)\u001b[39m\n\u001b[32m     30\u001b[39m lr_scheduler = ReduceLROnPlateau(monitor=\u001b[33m'\u001b[39m\u001b[33mval_loss\u001b[39m\u001b[33m'\u001b[39m, factor=\u001b[32m0.2\u001b[39m, patience=\u001b[32m2\u001b[39m, verbose=\u001b[32m0\u001b[39m)\n\u001b[32m     31\u001b[39m early_stop = EarlyStopping(monitor=\u001b[33m'\u001b[39m\u001b[33mval_loss\u001b[39m\u001b[33m'\u001b[39m, patience=\u001b[32m4\u001b[39m, restore_best_weights=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m history = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_generator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m                    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_generator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m                    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m                    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     37\u001b[39m \u001b[43m                    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlr_scheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stop\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m history.history[\u001b[33m'\u001b[39m\u001b[33mval_accuracy\u001b[39m\u001b[33m'\u001b[39m][-\u001b[32m1\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Cd Project Soil/soil/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:117\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    115\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Cd Project Soil/soil/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py:371\u001b[39m, in \u001b[36mTensorFlowTrainer.fit\u001b[39m\u001b[34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[39m\n\u001b[32m    369\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator:\n\u001b[32m    370\u001b[39m     callbacks.on_train_batch_begin(step)\n\u001b[32m--> \u001b[39m\u001b[32m371\u001b[39m     logs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    372\u001b[39m     callbacks.on_train_batch_end(step, logs)\n\u001b[32m    373\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.stop_training:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Cd Project Soil/soil/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py:220\u001b[39m, in \u001b[36mTensorFlowTrainer._make_function.<locals>.function\u001b[39m\u001b[34m(iterator)\u001b[39m\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[32m    217\u001b[39m     iterator, (tf.data.Iterator, tf.distribute.DistributedIterator)\n\u001b[32m    218\u001b[39m ):\n\u001b[32m    219\u001b[39m     opt_outputs = multi_step_on_iterator(iterator)\n\u001b[32m--> \u001b[39m\u001b[32m220\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mopt_outputs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhas_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    221\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m opt_outputs.get_value()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Cd Project Soil/soil/lib/python3.12/site-packages/tensorflow/python/data/ops/optional_ops.py:176\u001b[39m, in \u001b[36m_OptionalImpl.has_value\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m    174\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mhas_value\u001b[39m(\u001b[38;5;28mself\u001b[39m, name=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    175\u001b[39m   \u001b[38;5;28;01mwith\u001b[39;00m ops.colocate_with(\u001b[38;5;28mself\u001b[39m._variant_tensor):\n\u001b[32m--> \u001b[39m\u001b[32m176\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgen_optional_ops\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptional_has_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    177\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_variant_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\n\u001b[32m    178\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Cd Project Soil/soil/lib/python3.12/site-packages/tensorflow/python/ops/gen_optional_ops.py:172\u001b[39m, in \u001b[36moptional_has_value\u001b[39m\u001b[34m(optional, name)\u001b[39m\n\u001b[32m    170\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tld.is_eager:\n\u001b[32m    171\u001b[39m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m     _result = \u001b[43mpywrap_tfe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTFE_Py_FastPathExecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[43m      \u001b[49m\u001b[43m_ctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mOptionalHasValue\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptional\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    174\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[32m    175\u001b[39m   \u001b[38;5;28;01mexcept\u001b[39;00m _core._NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "bounds = [\n",
    "    [0.00005, 0.001],   # learning rate (slightly smaller upper bound)\n",
    "    [64, 128],          # conv1 filters\n",
    "    [128, 256],         # conv2 filters\n",
    "    [256, 512]          # dense units\n",
    "]\n",
    "best_params, best_score, convergence = dragonfly_optimize(bounds) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb5ae968-3746-43b3-97e8-470becc6583c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/simarkalsi/Cd Project Soil/soil/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "\u001b[1m171/171\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m120s\u001b[0m 686ms/step - accuracy: 0.6308 - loss: 10.7043 - val_accuracy: 0.2103 - val_loss: 18.0857 - learning_rate: 9.1184e-05\n",
      "Epoch 2/25\n",
      "\u001b[1m171/171\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 680ms/step - accuracy: 0.7736 - loss: 9.9403 - val_accuracy: 0.4359 - val_loss: 25.9603 - learning_rate: 9.1184e-05\n",
      "Epoch 3/25\n",
      "\u001b[1m171/171\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 685ms/step - accuracy: 0.8200 - loss: 10.5384 - val_accuracy: 0.7966 - val_loss: 8.0468 - learning_rate: 9.1184e-05\n",
      "Epoch 4/25\n",
      "\u001b[1m171/171\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 683ms/step - accuracy: 0.8448 - loss: 9.4066 - val_accuracy: 0.8675 - val_loss: 6.2227 - learning_rate: 9.1184e-05\n",
      "Epoch 5/25\n",
      "\u001b[1m171/171\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 687ms/step - accuracy: 0.8601 - loss: 10.7554 - val_accuracy: 0.8513 - val_loss: 11.5986 - learning_rate: 9.1184e-05\n",
      "Epoch 6/25\n",
      "\u001b[1m171/171\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 687ms/step - accuracy: 0.8827 - loss: 8.3663 - val_accuracy: 0.9265 - val_loss: 2.5664 - learning_rate: 9.1184e-05\n",
      "Epoch 7/25\n",
      "\u001b[1m171/171\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 687ms/step - accuracy: 0.8930 - loss: 7.8861 - val_accuracy: 0.9291 - val_loss: 3.9724 - learning_rate: 9.1184e-05\n",
      "Epoch 8/25\n",
      "\u001b[1m171/171\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 684ms/step - accuracy: 0.9065 - loss: 8.0939 - val_accuracy: 0.9265 - val_loss: 4.7217 - learning_rate: 9.1184e-05\n",
      "Epoch 9/25\n",
      "\u001b[1m171/171\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 577ms/step - accuracy: 0.9076 - loss: 8.1002\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 1.823682978283614e-05.\n",
      "\u001b[1m171/171\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 688ms/step - accuracy: 0.9076 - loss: 8.1005 - val_accuracy: 0.9350 - val_loss: 4.4327 - learning_rate: 9.1184e-05\n",
      "Epoch 10/25\n",
      "\u001b[1m171/171\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 685ms/step - accuracy: 0.9322 - loss: 6.3183 - val_accuracy: 0.9752 - val_loss: 1.1625 - learning_rate: 1.8237e-05\n",
      "Epoch 11/25\n",
      "\u001b[1m171/171\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 687ms/step - accuracy: 0.9460 - loss: 5.4233 - val_accuracy: 0.9744 - val_loss: 1.1782 - learning_rate: 1.8237e-05\n",
      "Epoch 12/25\n",
      "\u001b[1m171/171\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 690ms/step - accuracy: 0.9467 - loss: 4.5730 - val_accuracy: 0.9709 - val_loss: 1.1369 - learning_rate: 1.8237e-05\n",
      "Epoch 13/25\n",
      "\u001b[1m171/171\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 687ms/step - accuracy: 0.9453 - loss: 4.8777 - val_accuracy: 0.9735 - val_loss: 1.4629 - learning_rate: 1.8237e-05\n",
      "Epoch 14/25\n",
      "\u001b[1m171/171\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 689ms/step - accuracy: 0.9462 - loss: 5.2564 - val_accuracy: 0.9675 - val_loss: 1.5507 - learning_rate: 1.8237e-05\n",
      "Epoch 15/25\n",
      "\u001b[1m171/171\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 580ms/step - accuracy: 0.9544 - loss: 4.1491\n",
      "Epoch 15: ReduceLROnPlateau reducing learning rate to 3.647365883807652e-06.\n",
      "\u001b[1m171/171\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 691ms/step - accuracy: 0.9544 - loss: 4.1546 - val_accuracy: 0.9718 - val_loss: 1.2867 - learning_rate: 1.8237e-05\n",
      "Epoch 16/25\n",
      "\u001b[1m171/171\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 690ms/step - accuracy: 0.9446 - loss: 4.6466 - val_accuracy: 0.9718 - val_loss: 1.3697 - learning_rate: 3.6474e-06\n",
      "Epoch 17/25\n",
      "\u001b[1m171/171\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m119s\u001b[0m 689ms/step - accuracy: 0.9607 - loss: 3.8434 - val_accuracy: 0.9786 - val_loss: 1.0054 - learning_rate: 3.6474e-06\n",
      "Epoch 18/25\n",
      "\u001b[1m171/171\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 691ms/step - accuracy: 0.9627 - loss: 3.0364 - val_accuracy: 0.9829 - val_loss: 0.8684 - learning_rate: 3.6474e-06\n",
      "Epoch 19/25\n",
      "\u001b[1m171/171\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m119s\u001b[0m 691ms/step - accuracy: 0.9573 - loss: 3.5500 - val_accuracy: 0.9761 - val_loss: 0.7687 - learning_rate: 3.6474e-06\n",
      "Epoch 20/25\n",
      "\u001b[1m171/171\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 692ms/step - accuracy: 0.9618 - loss: 3.7354 - val_accuracy: 0.9803 - val_loss: 0.8231 - learning_rate: 3.6474e-06\n",
      "Epoch 21/25\n",
      "\u001b[1m171/171\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m119s\u001b[0m 692ms/step - accuracy: 0.9571 - loss: 4.0754 - val_accuracy: 0.9821 - val_loss: 0.7113 - learning_rate: 3.6474e-06\n",
      "Epoch 22/25\n",
      "\u001b[1m171/171\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 689ms/step - accuracy: 0.9579 - loss: 4.2073 - val_accuracy: 0.9803 - val_loss: 0.8755 - learning_rate: 3.6474e-06\n",
      "Epoch 23/25\n",
      "\u001b[1m171/171\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 691ms/step - accuracy: 0.9543 - loss: 4.7644 - val_accuracy: 0.9821 - val_loss: 0.7080 - learning_rate: 3.6474e-06\n",
      "Epoch 24/25\n",
      "\u001b[1m171/171\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 689ms/step - accuracy: 0.9581 - loss: 4.0682 - val_accuracy: 0.9761 - val_loss: 1.0832 - learning_rate: 3.6474e-06\n",
      "Epoch 25/25\n",
      "\u001b[1m171/171\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 693ms/step - accuracy: 0.9569 - loss: 4.4040 - val_accuracy: 0.9829 - val_loss: 0.6815 - learning_rate: 3.6474e-06\n"
     ]
    }
   ],
   "source": [
    "# === Final Training ===\n",
    "lr, f1, f2, dense = best_params\n",
    "f1, f2, dense = int(f1), int(f2), int(dense)\n",
    "\n",
    "final_model = models.Sequential([\n",
    "    layers.Conv2D(f1, (3, 3), activation='relu', padding='same', input_shape=(img_height, img_width, 3)),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.MaxPooling2D(2, 2),\n",
    "\n",
    "    layers.Conv2D(f2, (3, 3), activation='relu', padding='same'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.MaxPooling2D(2, 2),\n",
    "\n",
    "    layers.Conv2D(f2*2, (3, 3), activation='relu', padding='same'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.MaxPooling2D(2, 2),\n",
    "\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(dense, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "final_model.compile(optimizer=optimizers.Adam(learning_rate=lr),\n",
    "                    loss='categorical_crossentropy',\n",
    "                    metrics=['accuracy'])\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, verbose=1)\n",
    "\n",
    "history = final_model.fit(train_generator, \n",
    "                          validation_data=val_generator,\n",
    "                          epochs=25,\n",
    "                          verbose=1,\n",
    "                          callbacks=[early_stop, lr_scheduler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc3dfda3-f892-4bc3-bb23-dab82775c6c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m37/37\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 551ms/step - accuracy: 0.9767 - loss: 1.6862\n",
      "Test Accuracy: 0.9744\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on test set\n",
    "test_loss, test_acc = final_model.evaluate(test_generator)\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec0d155f-cb86-4eab-8b36-57a4094c8cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model.save(\"dragonfly97.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa1e899-7b93-4892-a602-54b8cc90160d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "soil",
   "language": "python",
   "name": "soil"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
